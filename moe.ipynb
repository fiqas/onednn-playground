{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7004a127-a4dc-4522-95d8-0ec465915725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from itertools import count\n",
    "import sys\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79a0e9f-1103-4d91-b14e-b8d54a717790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expert_num = 8 # number of experts\n",
    "# expert_dim = 3072\n",
    "# emb_dim = 768 # embedding size\n",
    "# token_num = 24576 * 4 # think token count, from unrolled batches possibly\n",
    "\n",
    "expert_num = 5 # number of experts\n",
    "expert_dim = 7\n",
    "emb_dim = 11 # embedding size\n",
    "token_num = 15 # think token count, from unrolled batches possibly\n",
    "\n",
    "# Bypass weight\n",
    "b = 0.1\n",
    "\n",
    "# Export sizes and bypass weight to C++ code as well\n",
    "exported = dict(\n",
    "    expert_count=expert_num,\n",
    "    expert_size=expert_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    token_count=token_num,\n",
    "    b=np.array([b], dtype=np.float32) # I don't care that those numbers are longs, but I want this multiplier to be float32.\n",
    ")\n",
    "\n",
    "# token count * embedding size\n",
    "words = randn(token_num, emb_dim).astype(np.float32, order='C')\n",
    "exported[\"src\"] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae5abfb-78af-4097-a3cd-891ee42a7133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expert (3) * emb_size (5) * emb_size (5)\n",
    "experts_w1 = randn(expert_num, emb_dim, expert_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_w1\"] = experts_w1\n",
    "\n",
    "# expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "experts_b1 = randn(expert_num, 1, expert_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_b1\"] = experts_b1\n",
    "\n",
    "# expert (3) * emb_size (5) * emb_size (5)\n",
    "experts_w2 = randn(expert_num, expert_dim, emb_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_w2\"] = experts_w2\n",
    "\n",
    "# expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "experts_b2 = randn(expert_num, 1, emb_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_b2\"] = experts_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0525a698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "(5, 15)\n",
      "[[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Generate overloaded top-1 router\n",
    "# Expert 0 will have half of all tokens in a batch\n",
    "# Other experts will split the rest equally among themselves\n",
    "\n",
    "router = np.zeros((expert_num, token_num)).astype(np.float32)\n",
    "\n",
    "# How many of all tokens will go to the overloaded expert\n",
    "ratio = 0.25\n",
    "\n",
    "# Fill it with ones, then shuffle.+++++ \n",
    "tokens_per_expert = np.int32(np.ceil((1 - ratio) * token_num / (expert_num - 1)))\n",
    "overloaded_expert = np.int32(np.ceil(ratio * token_num))\n",
    "\n",
    "print(tokens_per_expert, overloaded_expert)\n",
    "\n",
    "shift = 0\n",
    "for e in range(expert_num):\n",
    "    if e == 0:\n",
    "        for t in range(overloaded_expert):    \n",
    "            router[e][t + shift] = 1\n",
    "        shift += overloaded_expert\n",
    "    else:\n",
    "        for t in range(tokens_per_expert): \n",
    "            if t + shift >= token_num:\n",
    "                break\n",
    "            router[e][t + shift] = 1\n",
    "        shift += tokens_per_expert\n",
    "    \n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# print(router)\n",
    "\n",
    "#np.random.shuffle(np.transpose(router))\n",
    "\n",
    "print(router.shape)\n",
    "print(router)\n",
    "print(np.sum(router, axis=0))\n",
    "\n",
    "exported[\"router\"] = router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7238781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 15)\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Generate random binary router\n",
    "\n",
    "router = np.zeros((token_num, expert_num)).astype(np.float32)\n",
    "\n",
    "# Fill it with ones with each expert getting token_num / expert_num amount of tokens in it. Then shuffle.\n",
    "tokens_per_expert = token_num / expert_num\n",
    "\n",
    "counter = 0\n",
    "shift = 0\n",
    "expert = 0\n",
    "for t in range(token_num):\n",
    "    router[t][expert + shift] = 1\n",
    "    counter += 1\n",
    "    if counter == tokens_per_expert:\n",
    "        counter = 0\n",
    "        shift += 1\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(router)\n",
    "\n",
    "np.random.shuffle(router)\n",
    "router = np.transpose(router)\n",
    "print(router.shape)\n",
    "print(router)\n",
    "print(np.sum(router, axis=0))\n",
    "\n",
    "exported[\"router\"] = router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1a3256-64a7-4dbe-b1d4-ea86d875ea27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70429174-2e92-44b1-8bf0-e4aed9a6a890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0:\n",
      "  (3, 11) * (11, 7) + (1, 7) = (3, 7)\n",
      "\n",
      "  (3, 7) * (7, 11) + (1, 11) = (3, 11)\n",
      "\n",
      "Expert 1:\n",
      "  (3, 11) * (11, 7) + (1, 7) = (3, 7)\n",
      "\n",
      "  (3, 7) * (7, 11) + (1, 11) = (3, 11)\n",
      "\n",
      "Expert 2:\n",
      "  (3, 11) * (11, 7) + (1, 7) = (3, 7)\n",
      "\n",
      "  (3, 7) * (7, 11) + (1, 11) = (3, 11)\n",
      "\n",
      "Expert 3:\n",
      "  (3, 11) * (11, 7) + (1, 7) = (3, 7)\n",
      "\n",
      "  (3, 7) * (7, 11) + (1, 11) = (3, 11)\n",
      "\n",
      "Expert 4:\n",
      "  (3, 11) * (11, 7) + (1, 7) = (3, 7)\n",
      "\n",
      "  (3, 7) * (7, 11) + (1, 11) = (3, 11)\n",
      "\n",
      "Total output: (15, 11)\n"
     ]
    }
   ],
   "source": [
    "b = 0.1\n",
    "\n",
    "# words(7) * emb_size(4)\n",
    "total_output = b * words # â€¦ so that if a word doesn't go through any expert it will still have some value\n",
    "\n",
    "# Repeat for every expert, looking at routed do determine which words go to said expert\n",
    "for n, expert_w1, expert_b1, expert_w2, expert_b2, mask in zip(count(), experts_w1, experts_b1, experts_w2, experts_b2, router):\n",
    "    print(f\"Expert {n}:\")\n",
    "    \n",
    "    # select all words where the mask for this expert is > 0\n",
    "    expert_input = words[mask.nonzero()]\n",
    "    \n",
    "    # classic matmul op for feed-forward I guess? + relu\n",
    "    expert_output1 = np.maximum(0.0, np.add(np.matmul(expert_input, expert_w1), expert_b1))\n",
    "    expert_output2 = np.maximum(0.0, np.add(np.matmul(expert_output1, expert_w2), expert_b2))\n",
    "\n",
    "    # I did assignment here, but could also be addition\n",
    "    total_output[mask.nonzero()] = expert_output2\n",
    "\n",
    "    print(f\"  {expert_input.shape} * {expert_w1.shape} + {expert_b1.shape} = {expert_output1.shape}\\n\")\n",
    "    print(f\"  {expert_output1.shape} * {expert_w2.shape} + {expert_b2.shape} = {expert_output2.shape}\\n\")\n",
    "    \n",
    "    exported[f\"expert_{n}_src\"] = expert_input\n",
    "    exported[f\"expert_{n}_dst\"] = expert_output2\n",
    "    \n",
    "print(f\"Total output: {total_output.shape}\")\n",
    "exported[\"dst\"] = total_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261b8f50-16b9-456d-be41-43a568d3b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"data-toy-top-1-uniform.npz\", **exported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0aba73c-9359-4c69-878e-dcce81d278c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts_w2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec766a4-922e-4e1d-95af-01209aceceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_128\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_256\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_512\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_128\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_256\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_512\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_256_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_128\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_256\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_512\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_128\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_256\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_512\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_512_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_128\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_256\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_512\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_128\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_256\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_512\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_1024_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_128\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_256\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_512\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_128\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_256\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_512\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_2048_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_128\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_256\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_512\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_128\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_256\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_512\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_8_exp-dim_4096_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_128\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_256\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_512\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_128\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_256\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_512\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_256_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_128\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_256\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_512\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_128\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_256\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_512\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_512_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_128\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_256\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_512\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_128\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_256\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_512\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_1024_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_128\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_256\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_512\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_128\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_256\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_512\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_2048_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_128\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_256\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_512\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_128\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_256\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_512\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_16_exp-dim_4096_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_128\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_256\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_512\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_128\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_256\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_512\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_2048\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_4096\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_512_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_128\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_256\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_512\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_2048\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_4096\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_1024_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_128\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_256\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_512\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_2048\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_4096\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_2048_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_128\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_256\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_512\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_2048\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_4096\n",
      "Finished  exp-num_32_exp-dim_256_emb-dim_4096_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_128\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_256\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_512\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_2048\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_4096\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_256_token-num_8192\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_512_token-num_128\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_512_token-num_256\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_512_token-num_512\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_512_token-num_1024\n",
      "Finished  exp-num_32_exp-dim_512_emb-dim_512_token-num_2048\n"
     ]
    }
   ],
   "source": [
    "# Do grid search generation for various variable values. Save them to separate files.\n",
    "# Do not run automatically LOL\n",
    "\n",
    "expert_num_list = [8, 16, 32, 64, 128, 256] # number of experts\n",
    "expert_dim_list = [256, 512, 1024, 2048, 4096]\n",
    "emb_dim_list = [256, 512, 1024, 2048, 4096] # embedding size\n",
    "token_num_list = [128, 256, 512, 1024, 2048, 4096, 8192] # think token count, from unrolled batches possibly\n",
    "\n",
    "\n",
    "for expert_num in expert_num_list:\n",
    "    for expert_dim in expert_dim_list:\n",
    "        for emb_dim in emb_dim_list:\n",
    "            for token_num in token_num_list:\n",
    "                \n",
    "                exported = dict()\n",
    "                \n",
    "                # Generate input\n",
    "                words = randn(token_num, emb_dim).astype(np.float32)\n",
    "                exported[\"src\"] = words\n",
    "                \n",
    "                # Generate parameters\n",
    "                experts_w1 = randn(expert_num, emb_dim, expert_dim).astype(np.float32)\n",
    "                exported[\"experts_w1\"] = experts_w1\n",
    "\n",
    "                # expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "                experts_b1 = randn(expert_num, 1, expert_dim).astype(np.float32)\n",
    "                exported[\"experts_b1\"] = experts_b1\n",
    "\n",
    "                # expert (3) * emb_size (5) * emb_size (5)\n",
    "                experts_w2 = randn(expert_num, expert_dim, emb_dim).astype(np.float32)\n",
    "                exported[\"experts_w2\"] = experts_w2\n",
    "\n",
    "                # expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "                experts_b2 = randn(expert_num, 1, emb_dim).astype(np.float32)\n",
    "                exported[\"experts_b2\"] = experts_b2\n",
    "\n",
    "                b = 0.1\n",
    "\n",
    "                # words(7) * emb_size(4)\n",
    "                total_output = b * words # â€¦ so that if a word doesn't go through any expert it will still have some value\n",
    "\n",
    "                # Repeat for every expert, looking at routed do determine which words go to said expert\n",
    "                for n, expert_w1, expert_b1, expert_w2, expert_b2, mask in zip(count(), experts_w1, experts_b1, experts_w2, experts_b2, router):\n",
    "                    #print(f\"Expert {n}:\")\n",
    "\n",
    "                    # select all words where the mask for this expert is > 0\n",
    "                    expert_input = words[mask.nonzero()]\n",
    "\n",
    "                    # classic matmul op for feed-forward I guess? + relu\n",
    "                    expert_output1 = np.maximum(0.0, np.add(np.matmul(expert_input, expert_w1), expert_b1))\n",
    "                    expert_output2 = np.maximum(0.0, np.add(np.matmul(expert_output1, expert_w2), expert_b2))\n",
    "\n",
    "                    # I did assignment here, but could also be addition\n",
    "                    total_output[mask.nonzero()] = expert_output2\n",
    "\n",
    "                    #print(f\"  {expert_input.shape} * {expert_w1.shape} + {expert_b1.shape} = {expert_output1.shape}\\n\")\n",
    "                    #print(f\"  {expert_output1.shape} * {expert_w2.shape} + {expert_b2.shape} = {expert_output2.shape}\\n\")\n",
    "\n",
    "                    exported[f\"expert_{n}_src\"] = expert_input\n",
    "                    exported[f\"expert_{n}_dst\"] = expert_output2\n",
    "\n",
    "                #print(f\"Total output: {total_output.shape}\")\n",
    "                exported[\"dst\"] = total_output\n",
    "                \n",
    "                file_name = \"exp-num_\" + str(expert_num) + \"_exp-dim_\" + str(expert_dim) + \"_emb-dim_\" + str(emb_dim) + \"_token-num_\" + str(token_num)\n",
    "                np.savez_compressed(file_name + \".npz\", **exported)\n",
    "                print(\"Finished \", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e150e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
