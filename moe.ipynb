{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7004a127-a4dc-4522-95d8-0ec465915725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from itertools import count\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d79a0e9f-1103-4d91-b14e-b8d54a717790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expert_size = 3 # number of experts\n",
    "emb_size = 5 # embedding size\n",
    "emb_prime_size = 4 # internal expert size\n",
    "word_size = 7 # think token count, from unrolled batches possibly\n",
    "\n",
    "# token count * embedding size\n",
    "words = randn(word_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1ae5abfb-78af-4097-a3cd-891ee42a7133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expert (3) * emb_size (5) * emb_prime_size (4)\n",
    "experts_w1 = randn(expert_size, emb_size, emb_prime_size)\n",
    "\n",
    "# expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "experts_w2 = randn(expert_size, emb_prime_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dada2a0a-918b-4964-81b0-41414e7e4947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experts (3) * tokens\n",
    "routed = np.array([\n",
    "    [1, 1, 0, 0, 0, 0, 0], # first two tokens go to expert 1\n",
    "    [0, 0, 1, 1, 0, 0, 0], # second two tokens go to expert 2\n",
    "    [0, 0, 0, 0, 1, 1, 0]  # the very last token doesn't go to any expert(!)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "70429174-2e92-44b1-8bf0-e4aed9a6a890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0:\n",
      "  (2, 5) * (5, 4) * (4, 5) = (2, 5)\n",
      "\n",
      "Expert 1:\n",
      "  (2, 5) * (5, 4) * (4, 5) = (2, 5)\n",
      "\n",
      "Expert 2:\n",
      "  (2, 5) * (5, 4) * (4, 5) = (2, 5)\n",
      "\n",
      "Total output: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "b = 0.1\n",
    "\n",
    "# words(7) * emb_size(4)\n",
    "total_output = b * words # â€¦ so that if a word doesn't go through any expert it will still have some value\n",
    "\n",
    "# Repeat for every expert, looking at routed do determine which words go to said expert\n",
    "for n, expert_w1, expert_w2, mask in zip(count(), experts_w1, experts_w2, routed):\n",
    "    print(f\"Expert {n}:\")\n",
    "    \n",
    "    # select all words where the mask for this expert is > 0\n",
    "    expert_input = words[mask.nonzero()]\n",
    "    \n",
    "    expert_temp = np.matmul(expert_input, expert_w1)\n",
    "    expert_output = np.matmul(expert_temp, expert_w2)\n",
    "\n",
    "    # I did assignment here, but could also be addition\n",
    "    total_output[mask.nonzero()] = expert_output\n",
    "\n",
    "    print(f\"  {expert_input.shape} * {expert_w1.shape} * {expert_w2.shape} = {expert_output.shape}\\n\")\n",
    "\n",
    "print(f\"Total output: {total_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b8f50-16b9-456d-be41-43a568d3b4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
